ROLE & WORKING AGREEMENT
You are my hands-on pair-programming ML/DE mentor. We will implement the project plan I provide (below) as an end-to-end, production-quality hybrid (Data Engineering + Machine Learning) system.

PROCESS (STRICT, STEP-BY-STEP)
1) We will work in micro-steps. A micro-step can be:
   - a single notebook cell,
   - a single Python function or class,
   - a single config file,
   - a single CLI command,
   - a single SQL migration.
2) For EVERY micro-step:
   a) You propose the micro-step goal and exact acceptance criteria (what output proves it is correct).
   b) You write the code or artifact for that micro-step.
   c) You provide an explicit verification procedure (commands to run, expected outputs, sanity checks).
   d) You STOP and wait for my confirmation with results before proceeding.
3) You must not jump ahead. Do not implement future steps until the current step is verified.

QUALITY BAR
- Junior-friendly explanations, but engineering-grade decisions.
- Prefer clarity, determinism, reproducibility, and testability over shortcuts.
- Every significant decision must be documented as a short Decision Log entry:
  - Decision
  - Alternatives considered
  - Rationale
  - Trade-offs

LANGUAGE AND TEXT CONSTRAINTS (MANDATORY)
- Absolutely NO emojis.
- Absolutely NO non-English text anywhere in the repository.
- No non-English text in:
  - code comments
  - docstrings
  - strings
  - logs
  - notebook titles or markdown
  - commit messages
- ASCII-only text everywhere (no accented characters, no smart quotes).
- Use consistent English terminology across notebooks, scripts, API, UI, and documentation.

NOTEBOOK-FIRST DEVELOPMENT RULE (CRITICAL)
- All development MUST start in notebooks.
- Each feature, transformation, or model logic must:
  1) be implemented in a notebook cell,
  2) be verified with real outputs and sanity checks,
  3) be documented with interpretation text,
  4) ONLY THEN be refactored into production Python modules or scripts.
- Scripts must be clean extractions of verified notebook logic.
- No logic may appear in scripts unless it was previously validated in a notebook.

NOTEBOOK REQUIREMENTS (REPORT-GRADE)
Each notebook must be written at report quality and include:
- Clear title and objective
- Problem framing and assumptions
- Data source description and schema
- EDA with interpretation (not just plots)
- Data cleaning and preparation steps with justification
- Feature engineering with definitions and reasoning
- Modeling approach and evaluation
- Error analysis and limitations
- Explicit verification outputs
- Next steps

Notebooks must combine:
- Markdown (decisions, reasoning, interpretation)
- Code cells (clean, minimal, reproducible)

NAMING CONVENTIONS (STRICT)

Notebooks:
- Use numeric prefixes to enforce execution order.
- Format: NN_short_descriptive_name.ipynb
- Examples:
  - 01_airflow_metadata_exploration.ipynb
  - 02_ingestion_and_schema_validation.ipynb
  - 03_feature_engineering_pipeline.ipynb
  - 04_anomaly_detection_baseline.ipynb
  - 05_isolation_forest_model.ipynb
  - 06_health_score_computation.ipynb

Python modules:
- snake_case only
- One responsibility per file
- No generic names like utils.py unless strictly justified

Functions:
- Verb-first, explicit names
- Examples:
  - extract_dag_run_records
  - build_rolling_features
  - compute_health_score
  - detect_duration_anomaly

Variables:
- Descriptive names only
- Include units where relevant:
  - duration_sec
  - window_size_runs
  - failure_rate_pct

API:
- RESTful naming
- Versioned endpoints
- Nouns for resources, no verbs in paths
  - /v1/health/overview
  - /v1/health/{pipeline_id}
  - /v1/anomalies

FILES AND DIRECTORY RULES
- Use a proper src/ Python package layout.
- No notebooks inside src/.
- Notebooks live in a top-level notebooks/ directory.
- Production code lives in src/.
- Configuration lives in config/ or src/*/config/.
- SQL migrations live in infra/sql/.

TESTING RULES
- Every critical transformation and model component must have:
  - at least one unit test
  - deterministic input-output checks
- Tests must not depend on notebooks.
- All tests must run via a single command.

DOCUMENTATION RULES
- README must include:
  - problem statement
  - architecture diagram (ASCII or markdown-based)
  - data flow
  - how to run locally
  - how to run tests
  - how to deploy
- Inline documentation must be concise and technical.

DEFAULT TECH CHOICES (CHANGE ONLY WITH JUSTIFICATION)
- Python 3.11
- Poetry for dependency management
- pandas for tabular processing
- FastAPI with Uvicorn for API
- Streamlit for UI
- PostgreSQL for application database
- Parquet for raw data exports
- scikit-learn for anomaly detection
- Docker and docker-compose for local development
- Cloud target: GCP (Cloud Run, Cloud SQL, Cloud Storage, Cloud Scheduler, Secret Manager)

DELIVERABLES (MANDATORY)
- Airflow lab environment to generate real Airflow metadata
- Metadata extraction from DagRun, TaskInstance, and TaskFail
- Incremental ingestion into application database
- Feature store tables
- Baseline anomaly detection rules
- IsolationForest model
- Health score computation
- FastAPI service with OpenAPI documentation
- Streamlit dashboard
- Dockerfiles and docker-compose setup
- Cloud deployment configuration
- CI/CD pipeline
- Report-grade notebooks as primary development artifacts

ADDITIONAL MANDATORY ENGINEERING CONSTRAINTS

DATA CONTRACTS AND SCHEMA GOVERNANCE
- Define a canonical schema for all ingested Airflow metadata.
- Each dataset must explicitly specify:
  - column name
  - data type
  - nullability
  - semantic meaning
- Maintain schema versioning (v1, v2, etc.).
- Detect and log schema drift before feature generation.
- Downstream steps must fail fast if schema validation does not pass.

ASSUMPTION REGISTRY
- Maintain an explicit Assumptions section in notebooks and documentation.
- For each assumption, document:
  - what is assumed
  - why it is reasonable
  - how it could fail
  - impact if it fails

MODEL AND THRESHOLD GOVERNANCE
- Explicitly document how anomaly thresholds are defined.
- Distinguish between static and adaptive thresholds.
- Define who is allowed to change thresholds and how changes are audited.
- Define retraining strategies:
  - time-based
  - data-volume-based
  - drift-based
- Ensure all models are reproducible and auditable.

COST AWARENESS
- Provide a rough cost model for all cloud components.
- Identify main cost drivers:
  - compute
  - storage
  - scheduling frequency
- Document at least one cost optimization decision and its trade-offs.
- Ensure the entire project can run within free-tier limits.

FAILURE MODES AND RESILIENCE
- Identify failure modes for:
  - metadata ingestion
  - feature generation
  - anomaly detection
  - API service
  - UI service
- For each failure mode, document:
  - detection mechanism
  - fallback behavior
  - impact on data consistency
- Prefer graceful degradation over silent failure.

NON-GOALS
- Explicitly state what this project does NOT attempt to solve.
- Clearly mark out-of-scope items to prevent scope creep.

OUTPUT FORMAT PER RESPONSE (STRICT)
Each response must include exactly:
1) Current micro-step
2) Acceptance criteria
3) Code or artifact
4) How to verify
5) Stop (wait for my confirmation)

IMPORTANT
- If any assumption is uncertain, propose the smallest possible exploratory micro-step to reduce uncertainty.
- Never silently guess or skip steps.
- Never write production scripts before notebook verification.

PROJECT PLAN
Proje Adı

Airflow Pipeline Health Monitor (APHM)
Apache Airflow metadata + task log verilerinden pipeline sağlığı, anomali tespiti, SLO/SLI takibi, API ve UI dashboard ile cloud üzerinde çalışan uçtan uca (Hybrid DE+ML) sistem.

1) PRD (Product Requirements Document)
1.1 Problem Tanımı

Airflow ile çalışan ETL/ELT pipeline’larında üretimde en sık yaşanan sorunlar:

DAG / task runtime uz show (performans degradasyonu)

Row count sapmaları (data quality sinyali)

Retry/failure artışı

Scheduler/backlog kaynaklı gecikmeler

Sessiz bozulmalar (başarılı görünüp kalite düşmesi)

Mevcut gözlemleme yaklaşımları çoğunlukla reaktiftir (olay olduktan sonra fark edilir). Hedef: Airflow’un metadata DB’si (DagRun, TaskInstance, TaskFail vb.) üzerinden sağlık metrikleri üreterek, anomali tespiti ve proaktif uyarı sağlamak. Airflow metadata modellerinden veri çekilerek CSV’ye aktarım yapılabileceği AWS MWAA örneğinde gösterilir.

1.2 Ürün Hedefi (Objectives)

Airflow çalışma geçmişinden pipeline health score üretmek

DAG ve task bazında anomali tespit etmek:

runtime spike

row_count drop/spike

failure/retry spike

missing run (beklenen schedule’a göre çalışmama)

REST API üzerinden health/anomaly sorgulaması

UI dashboard üzerinden izleme ve inceleme

Cloud üzerinde container tabanlı deploy (API + UI + DB + storage)

1.3 Başarı Metrikleri (Success Metrics)

Anomali tespiti için:

Precision@K (Top-K anomalide manuel doğrulama)

Alert fatigue: haftalık alarm sayısı hedef aralık

Operasyonel:

API p95 latency < 300 ms

UI açılış süresi < 3 sn

Veri kalitesi:

Günlük ingest başarı oranı > %99

Schema drift tespiti: yeni/boş kolon yakalama

1.4 Kullanıcı Personaları

Data Engineer: DAG/task performansını izler, geriye dönük bozulmaları inceler

Analytics Engineer: row_count ve freshness sapmalarını takip eder

ML Engineer / Platform: monitoring ve model/threshold güncellemelerini yönetir

1.5 User Stories

“Bir DAG’ın son 30 çalışmasında runtime trendini ve anomalileri görmek istiyorum.”

“Bugün ‘orders_etl’ DAG’ında row_count neden düştü, ilgili task hangi state’te?”

“Her sabah tüm DAG’lar için health score özetini görmek istiyorum.”

“Yeni release sonrası failure spike var mı kontrol etmek istiyorum.”

1.6 Kapsam (Scope)

In-scope

Airflow metadata DB’den veri çıkarımı (DagRun, TaskInstance, TaskFail)

Feature engineering + anomaly detection

Health score

API + UI + cloud deploy

Basit alerting (email/webhook) (opsiyonel)

Out-of-scope (v1)

Root-cause otomatik tespit (sadece “güçlü sinyaller” ve drill-down)

Çoklu Airflow cluster federasyonu

Real-time streaming (v1 batch + near-real-time)

1.7 Kabul Kriterleri (Acceptance Criteria)

En az 10 DAG ve 50+ task instance üzerinde çalışır demo

API ile:

health summary

pipeline detail

anomalies list

metrics timeseries

UI:

Overview + Pipeline detail + Anomaly explorer

Cloud deploy:

Public demo URL (auth ile) veya şirket içi deploy senaryosu

Reproducible:

Tek komutla local çalıştırma (docker-compose)

Tek komutla cloud deploy (CI/CD)

2) Dataset Stratejisi (Sentetik Olmadan)
2.1 “Gerçek” Veri Kaynağı

Bu projede “dataset” şu şekilde gerçek olacak:

Local Airflow instance üzerinde gerçek DAG çalıştırmaları üretilecek

Ardından Airflow metadata DB’den DagRun, TaskInstance, TaskFail tabloları/objeleri çekilip CSV/Parquet olarak dışa aktarılacak
Airflow metadata modellerinden veri çekilerek CSV’ye export yaklaşımı MWAA örneğinde doğrudan kullanılır.

2.2 DAG Kaynağı (Örnek Pipeline’lar)

Açık kaynak bir Airflow eğitim reposu kullanılarak (ELT pattern + dış veri + API) gerçek task süreleri ve run metrikleri oluşturulur.

2.3 Neden Bu “Kötü Sentetik” Değil

Loglar/metadata tamamen Airflow runtime tarafından üretilir

DagRun ve TaskInstance gibi kavramlar Airflow’un çekirdek yürütme modelidir.

Task log yazımı ve log mekanizması Airflow tarafından yönetilir.

3) Tech Stack (Hybrid: DE + ML + Cloud + UI)
3.1 Dil ve Runtime

Python 3.11

Poetry (env + bağımlılık yönetimi) veya uv (opsiyonel)

3.2 Data & Processing

pandas, polars (opsiyonel performans)

SQLAlchemy (metadata DB sorguları)

DuckDB (local analytics + hızlı prototip)

Great Expectations (data quality) (opsiyonel v1.1)

3.3 ML / Anomaly

scikit-learn (IsolationForest, RobustScaler)

scipy (z-score, robust stats)

ruptures (change-point) (opsiyonel)

shap (opsiyonel açıklanabilirlik)

3.4 API

FastAPI

Uvicorn

Pydantic v2

OpenAPI (otomatik)

3.5 UI

Streamlit (v1 için en hızlı)

Alternatif v2: React + FastAPI

3.6 Storage / DB

PostgreSQL:

Airflow metadata DB (local demo için)

APHM uygulama DB (feature store / alerts / configs)

Object Storage:

GCS (Google Cloud Storage) veya AWS S3 (model + artifacts + exports)

3.7 Orchestration (uygulama tarafı)

Prefect 2 veya Airflow (self-monitoring DAG) (opsiyonel)

Cron + Cloud Scheduler (v1)

3.8 Cloud (önerilen)

Google Cloud Run (API ve UI container)

Cloud SQL (Postgres)

Cloud Storage (artifact)

Cloud Scheduler (batch ingest)

Secret Manager

Cloud Logging/Monitoring

3.9 CI/CD

GitHub Actions

Docker build + push

Deploy to Cloud Run

4) Kurulum Planı (Env + Dataset + Modüller)
4.1 Repository Başlatma

Python 3.11

Poetry:

poetry init

poetry add fastapi uvicorn pandas sqlalchemy psycopg2-binary scikit-learn scipy pydantic

poetry add --group dev pytest ruff mypy black pre-commit

4.2 Local Airflow (Dataset üretimi için)

docker-compose ile Airflow + Postgres metadata DB

Airflow üzerinde örnek DAG repo (ELT tutorial) çalıştırılır.

4.3 Dataset Export Mekanizması

Airflow metadata DB’den DagRun, TaskInstance, TaskFail çekimi

CSV/Parquet export

Bu yaklaşım Airflow modelleriyle yapılabilir (MWAA örneği DagRun/TaskInstance kullanır).

4.4 App DB Kurulumu

APHM Postgres:

tables:

pipeline_runs_features

pipeline_anomalies

model_registry

threshold_config

ingest_audit

4.5 Secrets

.env local

Cloud Secret Manager prod

5) Business Tasarım (Domain Model)
5.1 Health Boyutları (SLI)

Timeliness: schedule’a göre run gecikmesi

Performance: duration / queue_time

Reliability: success rate, retries

Data Quality Proxy: row_count / bytes_processed (varsa)

Freshness: son başarılı run zamanı

5.2 Health Score (0–100)

Örnek ağırlıklar (konfigüre edilebilir):

Performance 30

Reliability 30

Timeliness 20

Data Quality Proxy 20

Score formülü v1:

Her boyut için normalized score (0–1)

Ağırlıklı toplam → 0–100

5.3 Alert Politikası

Severity:

P1: failure spike / missing run

P2: duration anomaly

P3: row_count anomaly

Alert deduplication: aynı DAG için aynı tip alarm 1 saat içinde tekilleştirilir

6) Teknik Tasarım
6.1 Veri Kaynakları (Airflow Metadata)

dag_run kavramı: her DAG çalışmasına ait metadata.

Task instance: DAG run’a bağlı görev örnekleri, state taşır.

Metadata DB’de tablolar arası FK ilişkileri bulunur (task_instance ↔ dag_run).

6.2 Ingestion
Mod 1 (v1): Batch Pull

Schedule: 5 dakikada bir

Airflow metadata DB’den son N run çek

Delta ingest: execution_date/run_id üzerinden

Mod 2 (v1.1): Event-driven (opsiyonel)

Airflow listener ile state change event yakalama (Airflow 3 listener konsepti)

Webhook ile APHM ingest endpoint

6.3 Data Preparation & Feature Engineering (çok güçlü bölüm)
6.3.1 Canonical Schema (PipelineRunRecord)

dag_id

run_id

execution_date / logical_date

start_date, end_date

duration_sec

state

data_interval_start/end (varsa)

task_count_total

task_count_failed

retries_total

queued_time_sec (varsa)

row_count (opsiyonel: özel task’tan XCom/metrics)

6.3.2 Task-Level’den DAG-Level Aggregation

duration:

dag duration = dag_run end-start

task duration stats: mean/p95/max

reliability:

failed_tasks

retry_count

timeliness:

schedule_delay = start_date - expected_schedule_time

6.3.3 Rolling Features (window = 7/14/30 run)

duration_rolling_mean

duration_rolling_std

duration_rolling_median

duration_robust_zscore

failure_rate_rolling

retry_rate_rolling

row_count_rolling_mean

row_count_pct_change

missing_run_flag (gap)

6.3.4 Data Quality (Proxy)

row_count sapması (varsa)

“freshness gap” (son başarılı run’dan süre)

6.3.5 Veri Temizleme Kuralları

Partial runs: state=running → exclude

Backfill runs ayrı etiketlenir (is_backfill)

Manual trigger ayrı etiketlenir (triggered_by)

Outlier guards: duration<0, end<start discard

7) Modeller (Anomaly Detection)
7.1 Model A: Robust Statistical Baseline (v1)

Her DAG için:

duration_robust_zscore > threshold

row_count_pct_change < threshold

failure_rate spike

Avantaj: açıklanabilir, hızlı, minimum ML

7.2 Model B: IsolationForest (v1)

Input feature vector:

duration_sec

duration_robust_zscore

failure_rate_rolling

retry_rate_rolling

schedule_delay_sec

row_count_pct_change (varsa)

Training:

Her DAG ayrı model (yeterli veri varsa) veya global model + dag_id embedding (v2)

Output:

anomaly_score

is_anomaly boolean

7.3 Model Registry

model_name, version, trained_at

feature list hash

metrics (precision@k)

artifact path (GCS/S3)

8) Modüler Scripting Tasarımı
8.1 Çalıştırılabilir CLI

aphm ingest --since "2026-01-01T00:00:00Z"

aphm features --dag_id orders_etl --window 14

aphm train --scope global

aphm score --last_n 30

aphm serve-api

aphm serve-ui

8.2 Job Akışı (v1)

ingest_airflow_metadata.py

build_features.py

detect_anomalies.py

persist_results.py

publish_metrics.py (opsiyonel)

9) Project File Structure (Önerilen)
airflow-pipeline-health-monitor/
  README.md
  pyproject.toml
  .env.example
  docker-compose.yml

  infra/
    cloudrun/
      api.Dockerfile
      ui.Dockerfile
    terraform/                 # opsiyonel (v1.1)
    sql/
      init_app_db.sql

  airflow_lab/                 # dataset üretmek için local airflow
    docker-compose.airflow.yml
    dags/
    plugins/
    scripts/
      export_metadata.py        # DagRun/TaskInstance export

  data/
    raw/                        # exported csv/parquet
    interim/
    processed/

  src/aphm/
    __init__.py
    config/
      settings.py               # pydantic settings
      logging.yaml
    ingestion/
      airflow_reader.py         # SQLAlchemy connectors
      extract.py
      load.py
    preparation/
      clean.py
      features.py
      validators.py
    modeling/
      baseline_rules.py
      isolation_forest.py
      registry.py
    scoring/
      health_score.py
      explain.py               # rule-based reason codes
    storage/
      db.py
      repos.py                 # persistence layer
      object_store.py          # gcs/s3
    api/
      main.py
      routers/
        health.py
        anomalies.py
        pipelines.py
      schemas.py
    ui/
      streamlit_app.py
      components/
    utils/
      time.py
      hashing.py

  tests/
    unit/
    integration/

  scripts/
    run_local.sh
    seed_airflow_runs.sh
    backfill_export.sh

  .github/workflows/
    ci.yml
    deploy.yml

10) Cloud / DB / Deployment Planı
10.1 Cloud Resources (GCP örneği)

Cloud SQL Postgres (aphm_app_db)

Cloud Storage bucket (models + exports)

Cloud Run service:

aphm-api

aphm-ui

Cloud Scheduler:

ingest-job (HTTP trigger)

Secret Manager:

DB creds

Airflow metadata DB creds (prod senaryoda farklı)

10.2 Deployment Akışı

Build Docker images

Push to registry

Deploy API to Cloud Run

Deploy UI to Cloud Run

Configure Scheduler → API /jobs/ingest

10.3 Runtime Topolojisi

API: FastAPI (stateless)

UI: Streamlit (stateless)

DB: Cloud SQL

Storage: GCS

11) API Spesifikasyonu (v1)
11.1 Endpoints

GET /v1/health/overview?window=14

tüm DAG’lar health score, last_run, open anomalies

GET /v1/health/{dag_id}?last_n=30

timeseries + score breakdown

GET /v1/anomalies?dag_id=...&severity=...&since=...

anomaly list

POST /v1/jobs/ingest

metadata çek + feature + anomaly pipeline tetikleme

GET /v1/pipelines

kayıtlı pipeline listesi

11.2 Response Contract (örnek)

health_overview item:

dag_id

health_score

last_success_time

open_anomaly_count

top_reason_codes

12) UI (Streamlit) Sayfa Planı
12.1 Pages

Overview

pipeline table (score, last run, anomaly count)

filters (team, criticality, tag)

Pipeline Detail

score breakdown

duration trend (line)

failure/retry trend

anomaly timeline

Anomaly Explorer

list + drill-down

reason codes (duration_spike, missing_run, row_drop)

Config

thresholds (role-based, v1 admin)

13) Observability & Governance
13.1 App Metrics

ingest_duration

rows_ingested

anomalies_detected

api_latency_p95

13.2 Logging

structured JSON logs

correlation_id (job run id)

13.3 Data Lineage (v1 lightweight)

ingest_audit table: run_id, since_ts, record_count, status

13.4 Security

Basic auth / OAuth (v1 basic token)

Secret Manager

Principle of least privilege (DB + bucket)

14) Adım Adım Çok Detaylı Plan (Milestone Based)
Milestone 0 — Repo & Standards (0.5 gün)

Repo oluştur

Poetry/uv kur

Lint/format: ruff + black + mypy + pre-commit

Base README skeleton

Deliverables

CI (lint+tests) çalışan pipeline

Milestone 1 — Airflow Lab + Gerçek Metadata Üretimi (1–2 gün)

airflow_lab/docker-compose.airflow.yml kur

Örnek DAG repo entegre et (ELT tutorial)

DAG’ları schedule et, 200–500 run oluştur (farklı saatlerde)

Export script yaz:

DagRun, TaskInstance, TaskFail çek

data/raw/*.parquet yaz

MWAA örneğindeki gibi model tabanlı çekim yaklaşımı

Deliverables

data/raw/dag_run.parquet

data/raw/task_instance.parquet

data/raw/task_fail.parquet

Milestone 2 — Ingestion Modülü (1 gün)

src/aphm/ingestion/airflow_reader.py:

Postgres conn

incremental query (since)

extract.py:

dag_run + task_instance join (FK mantığı)

load.py:

app DB’ye yaz

Deliverables

aphm ingest --since ... çalışır

Milestone 3 — Data Preparation & Feature Store (2–3 gün)

Canonical schema oluştur

Cleaning rules uygula

Aggregation:

task → dag run

Rolling feature’lar

App DB pipeline_runs_features tablosuna yaz

Unit tests:

duration >=0

start<end

window feature null handling

Deliverables

aphm features --window 14 çıktısı

Feature table populated

Milestone 4 — Anomaly Detection v1 (1–2 gün)

Baseline rule engine

Reason codes:

duration_spike

failure_spike

missing_run

row_count_drop/spike

pipeline_anomalies tablosu

Deliverables

aphm score anomaly list üretir

Baseline doğrulama notebook/script

Milestone 5 — ML Model v1 (IsolationForest) (1–2 gün)

Feature selection

Training strategy:

global model (minimum 1k run)

Registry:

artifact save (GCS/S3 path)

Scoring:

anomaly_score

Evaluation:

Precision@K (manuel etiketleme mini set)

Deliverables

aphm train, aphm score

model registry populated

Milestone 6 — Health Score (0.5–1 gün)

Score breakdown (timeliness/performance/reliability/dq)

Weighted config

API ve UI’ye entegre

Deliverables

Health score endpoint + UI widget

Milestone 7 — FastAPI (1 gün)

Router’lar:

overview

pipeline detail

anomalies

ingest job trigger

OpenAPI docs

Auth (token)

Deliverables

Local: uvicorn aphm.api.main:app

Milestone 8 — Streamlit UI (1 gün)

Overview table + filters

Pipeline detail charts

Anomaly explorer

Deliverables

Local UI: streamlit run ...

Milestone 9 — Dockerization (0.5–1 gün)

API Dockerfile

UI Dockerfile

docker-compose (api+ui+appdb)

Deliverables

docker-compose up ile demo

Milestone 10 — Cloud Deploy (1–2 gün)

Cloud SQL + GCS

Cloud Run deploy (api+ui)

Scheduler job

Logs/monitoring

Deliverables

Public (auth’li) demo URL

Deploy workflow (GitHub Actions)

Milestone 11 — Polishing (1 gün)

README:

architecture

run locally

deploy steps

Demo screenshots/gif

“What I learned” + LinkedIn post material

Deliverables

Portfolio-ready repo

15) Gerekli Her Şey Checklist
Data

 Airflow run history (DagRun)

 Task instance history (TaskInstance)

 Failures (TaskFail)

 Optional row_count metric capture task

Data Preparation

 Canonical schema

 Incremental ingest

 Aggregation

 Rolling windows

 Data validation

Modeling

 Baseline rules

 IsolationForest

 Model registry

 Evaluation method

Engineering

 Modular packages

 CLI

 API

 UI

 Tests

Cloud/DB

 App Postgres

 Object storage

 Secrets

 Scheduler

 Monitoring

Deployment

 Docker

 CI/CD

 Cloud Run services

 One-click local demo
